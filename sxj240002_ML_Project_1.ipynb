{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Unzipping folders(Please place folders into the working directory, outside sample_data)"
      ],
      "metadata": {
        "id": "1W0smE_YIydp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BJJWe_BkIliK"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "for i in [1, 2, 4]:\n",
        "  zip_file1=ZipFile(f\"enron{i}_train.zip\",'r')\n",
        "  zip_file1.extractall()\n",
        "  zip_file1.close()\n",
        "  zip_file1=ZipFile(f\"enron{i}_test.zip\",'r')\n",
        "  zip_file1.extractall()\n",
        "  zip_file1.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now manually make a folder for enron2 in the same way as enron1 and enron4 and place test and training for enron2 in there"
      ],
      "metadata": {
        "id": "A7Dhd9utKOwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Importing all the required libraries"
      ],
      "metadata": {
        "id": "7hVI8vOnKkpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqXoAACSKVel",
        "outputId": "fe0da0d4-4fca-4c3f-a137-2da02fbb698b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Bag of words model method"
      ],
      "metadata": {
        "id": "DPU7qtFBK9_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "from collections import Counter\n",
        "import glob\n",
        "import re\n",
        "from math import log10 as log\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "# This method is used to return the lists of strings from the txt files along with the sizes of those lists\n",
        "def ImportDataset(Dataset, isTraining):\n",
        "\n",
        "    hamFiles = []\n",
        "    spamFiles = []\n",
        "    totalFiles = \"\"\n",
        "\n",
        "    path = os.path.join(os.getcwd(), Dataset)\n",
        "\n",
        "    if isTraining == True:\n",
        "        path = os.path.join(path, \"train\")\n",
        "    else:\n",
        "        path = os.path.join(path, \"test\")\n",
        "\n",
        "    hamPaths=os.path.join(path,\"ham\")\n",
        "    spamPaths=os.path.join(path,\"spam\")\n",
        "\n",
        "    hamFileList = glob.glob(hamPaths + \"/\" + \"*.txt\")\n",
        "    spamFileList = glob.glob(spamPaths+ \"/\" + \"*.txt\")\n",
        "\n",
        "    for spamFile in spamFileList:\n",
        "        spamFiles.append(open(spamFile, \"r\", encoding='Latin-1').read())\n",
        "        totalFiles = totalFiles + \" \" + open(spamFile, \"r\",encoding='Latin-1').read()\n",
        "    for hamFile in hamFileList:\n",
        "        hamFiles.append(open(hamFile, \"r\",encoding='Latin-1').read())\n",
        "        totalFiles = totalFiles + \" \" + open(hamFile, \"r\",encoding='Latin-1').read()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    sizeOfTotalFileList = len(hamFileList) + len(spamFileList)\n",
        "    sizeOfHamList = len(hamFileList)\n",
        "    sizeOfSpamList = len(spamFileList)\n",
        "    return spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList\n",
        "\n",
        "#This method converts the list of strings into a bag of word representation for spam and ham datasets respectively\n",
        "def BagOfWords(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList):\n",
        "\n",
        "\n",
        "    vocabulary = {}\n",
        "    totalFrequencyList = {}\n",
        "    spamFrequencyList = []\n",
        "    spamVocab = {} #stores the spam files' word count\n",
        "    hamVocab = {}#stores the ham files' word count\n",
        "    hamFrequencyList = []\n",
        "\n",
        "\n",
        "    totalFilteredData=re.findall(\"[a-zA-Z]+\",totalFiles)\n",
        "    stopwordsList = stopwords.words('english')\n",
        "\n",
        "    for word in totalFilteredData:\n",
        "        word = word.lower()\n",
        "        if word in vocabulary:\n",
        "            continue\n",
        "        else:\n",
        "            if word not in stopwordsList:\n",
        "                vocabulary[word] = 0\n",
        "        if word in totalFrequencyList:\n",
        "            if word not in stopwordsList:\n",
        "                totalFrequencyList[word] = totalFrequencyList[word] + 1\n",
        "        else:\n",
        "            if word not in stopwordsList:\n",
        "                totalFrequencyList[word] = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # spam files bag of words\n",
        "    for email in spamFiles:\n",
        "        spamFilteredData=re.findall(\"[a-zA-Z]+\",email)\n",
        "        tempDictionary = copy.deepcopy(vocabulary)\n",
        "        for word in spamFilteredData:\n",
        "            word = word.lower()\n",
        "            if word in tempDictionary:\n",
        "                tempDictionary[word] = tempDictionary[word] + 1\n",
        "        spamVocab = Counter(spamVocab) + Counter(tempDictionary)\n",
        "        spamFrequencyList.append(tempDictionary)\n",
        "\n",
        "   # ham files bag of words\n",
        "    for email in hamFiles:\n",
        "        tempDictionary = copy.deepcopy(vocabulary)\n",
        "        hamFilteredData=re.findall(\"[a-zA-Z]+\",email)\n",
        "        for word in hamFilteredData:\n",
        "            word = word.lower()\n",
        "            if word in tempDictionary:\n",
        "                tempDictionary[word] = tempDictionary[word] + 1\n",
        "\n",
        "        hamVocab = Counter(hamVocab) + Counter(tempDictionary)\n",
        "        hamFrequencyList.append(tempDictionary)\n",
        "\n",
        "\n",
        "    return spamFrequencyList, hamFrequencyList, totalFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def BernoulliModel(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList):\n",
        "\n",
        "\n",
        "    vocabulary = {}\n",
        "    spamBernoulli = []\n",
        "    spamVocab = {}\n",
        "    hamBernoulli = []\n",
        "    hamVocab = {}\n",
        "\n",
        "    totalFilteredData=re.findall(\"[a-zA-Z]+\",totalFiles)\n",
        "    stopwordsList = stopwords.words('english')\n",
        "\n",
        "    for word in totalFilteredData:\n",
        "        word = word.lower()\n",
        "        if word in vocabulary:\n",
        "            continue\n",
        "        else:\n",
        "            if word not in stopwordsList:\n",
        "                vocabulary[word] = 0\n",
        "\n",
        "\n",
        "    for email in spamFiles:\n",
        "        tempDictionary = copy.deepcopy(vocabulary)\n",
        "        spamFilteredData=re.findall(\"[a-zA-Z]+\",email)\n",
        "        for word in spamFilteredData:\n",
        "            word = word.lower()\n",
        "            if word in tempDictionary:\n",
        "                tempDictionary[word] = 1\n",
        "                spamVocab[word] = 1\n",
        "        temp_list = list(tempDictionary.values())\n",
        "        spamBernoulli.append(tempDictionary)\n",
        "\n",
        "\n",
        "    for email in hamFiles:\n",
        "        tempDictionary = copy.deepcopy(vocabulary)\n",
        "        hamFilteredData=re.findall(\"[a-zA-Z]+\",email)\n",
        "        for word in hamFilteredData:\n",
        "            word = word.lower()\n",
        "            if word in tempDictionary:\n",
        "                tempDictionary[word] = 1\n",
        "                hamVocab[word] = 1\n",
        "        hamBernoulli.append(tempDictionary)\n",
        "    return spamBernoulli, hamBernoulli, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#method to train MNB\n",
        "def TrainMultinomialNB(spamFrequencyList, hamFrequencyList, totalFrequencyList,\n",
        "                         spamVocab, hamVocab, sizeOfTotalFileList,\n",
        "                         sizeOfSpamList, sizeOfHamList, vocabulary):\n",
        "\n",
        "\n",
        "\n",
        "    condProbability = {}\n",
        "    condProbability[\"spam\"] = {}\n",
        "    condProbability[\"ham\"] = {}\n",
        "    condProbabilityOfAbsentWord = {} #this is the conditional probabilities of words that may be absent from the spam/ham files in the training data\n",
        "    condProbabilityOfAbsentWord[\"spam\"] = {}\n",
        "    condProbabilityOfAbsentWord[\"ham\"] = {}\n",
        "    priors = {}\n",
        "\n",
        "\n",
        "\n",
        "    priors[\"spam\"] = log(sizeOfSpamList / float(sizeOfTotalFileList))\n",
        "    priors[\"ham\"] =log(sizeOfHamList / float(sizeOfTotalFileList))\n",
        "    totalSpamWords = sum(spamVocab.values())\n",
        "    totalHamWords = sum(hamVocab.values())\n",
        "\n",
        "    for word in list(spamVocab):\n",
        "        condProbability[\"spam\"][word] =log((spamVocab[word] + 1) / (float(totalSpamWords + len(totalFrequencyList))))\n",
        "\n",
        "\n",
        "    for word in list(hamVocab):\n",
        "        condProbability[\"ham\"][word] =log((hamVocab[word] + 1) / (\n",
        "            float(totalHamWords + len(totalFrequencyList))))\n",
        "\n",
        "    condProbabilityOfAbsentWord[\"ham\"] =log(1 / (float(totalHamWords + len(totalFrequencyList))))\n",
        "    condProbabilityOfAbsentWord[\"spam\"] =log(1 / (float(totalSpamWords + len(totalFrequencyList))))\n",
        "    return condProbability, condProbabilityOfAbsentWord, priors\n",
        "\n",
        "#method to test MNB, returns 1 if spam and -1 if ham\n",
        "def TestMultinomialNB(priors, condProbability, condProbabilityOfAbsentWord,\n",
        "                                 testingBOW):\n",
        "\n",
        "    score = {}\n",
        "    for _class in list(priors):\n",
        "        score[_class] = priors[_class]\n",
        "        for word in list(testingBOW):\n",
        "            if testingBOW[word] != 0:\n",
        "                try:\n",
        "                    score[_class] += condProbability[_class][word]\n",
        "                except KeyError:\n",
        "                    score[_class] += condProbabilityOfAbsentWord[_class]\n",
        "\n",
        "    if score[\"spam\"] > score[\"ham\"]:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def findAccuracy(predictedVal,trueVal):\n",
        "\n",
        "    correctPrediction = 0\n",
        "    for val in range(len(trueVal)):\n",
        "        if trueVal[val] == predictedVal[val]:\n",
        "            correctPrediction =correctPrediction+1\n",
        "    return correctPrediction / float(len(trueVal))\n",
        "\n",
        "\n",
        "def findPrecision(predictedVal,trueVal):\n",
        "\n",
        "    truePositives = 0\n",
        "    falsePositives = 0\n",
        "    for val in range(len(trueVal)):\n",
        "        if trueVal[val] == predictedVal[val] and predictedVal[val] == 1:\n",
        "            truePositives += 1\n",
        "        if trueVal[val] != predictedVal[val] and predictedVal[val] == 1:\n",
        "            falsePositives += 1\n",
        "    return truePositives / float(truePositives + falsePositives)\n",
        "\n",
        "\n",
        "def findRecall(predictedVal,trueVal):\n",
        "\n",
        "    truePositives = 0\n",
        "    falseNegatives = 0\n",
        "    for val in range(len(trueVal)):\n",
        "        if trueVal[val] == predictedVal[val] and predictedVal[val] == 1:\n",
        "            truePositives += 1\n",
        "        if trueVal[val] != predictedVal[val] and predictedVal[val] == 0:\n",
        "            falseNegatives += 1\n",
        "    return truePositives / float(truePositives + falseNegatives)\n",
        "\n",
        "\n",
        "def findF1_score(recall, precision):\n",
        "\n",
        "    return (2 *precision*recall) / float(recall + precision)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def TrainDiscreteNB(spamBernoulli, hamBernoulli,\n",
        "                               spamVocab, hamVocab, sizeOfTotalFileList,\n",
        "                               sizeOfSpamList, sizeOfHamList, vocabulary):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    condProbability = {}\n",
        "    condProbability[\"spam\"] = {}\n",
        "    condProbability[\"ham\"] = {}\n",
        "    condProbabilityOfAbsentWord = {}\n",
        "    condProbabilityOfAbsentWord[\"spam\"] = {}\n",
        "    condProbabilityOfAbsentWord[\"ham\"] = {}\n",
        "    priors = {}\n",
        "\n",
        "    priors[\"spam\"] =log(sizeOfSpamList/ float(sizeOfTotalFileList))\n",
        "    priors[\"ham\"] =log(sizeOfHamList / float(sizeOfTotalFileList))\n",
        "\n",
        "\n",
        "    for word in spamVocab:\n",
        "        condProbability[\"spam\"][word] =log(\n",
        "            1 + spamVocab[word] / (float(sizeOfSpamList+ 2)))\n",
        "\n",
        "    for word in hamVocab:\n",
        "        condProbability[\"ham\"][word] =log(\n",
        "            1 + hamVocab[word] / float(sizeOfHamList + 2))\n",
        "\n",
        "    condProbabilityOfAbsentWord[\"ham\"] =log(1 / (float(sizeOfHamList + 2)))\n",
        "    condProbabilityOfAbsentWord[\"spam\"] =log(1 / (float(sizeOfSpamList+ 2)))\n",
        "    return condProbability, condProbabilityOfAbsentWord, priors\n",
        "\n",
        "\n",
        "def TestDiscreteNB(priors, condProbability, condProbabilityOfAbsentWord,\n",
        "                              testingBernoulli):\n",
        "\n",
        "    score = {}\n",
        "\n",
        "    for _class in list(priors):\n",
        "        score[_class] = priors[_class]\n",
        "        for word in list(testingBernoulli):\n",
        "            if testingBernoulli[word] != 0:\n",
        "                try:\n",
        "                    score[_class] += condProbability[_class][word]\n",
        "\n",
        "                except KeyError:\n",
        "                    score[_class] += condProbabilityOfAbsentWord[_class]\n",
        "\n",
        "    if score[\"spam\"] > score[\"ham\"]:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#to initialize W0 value and document class values\n",
        "\n",
        "import random\n",
        "\n",
        "def InitializeW0(spamMails,hamMails):\n",
        "    for doc in spamMails:\n",
        "        doc[\"W0\"] = 1\n",
        "        doc[\"DocumentClass\"]=1\n",
        "    for doc in hamMails:\n",
        "        doc[\"W0\"] = 1\n",
        "        doc[\"DocumentClass\"]=0\n",
        "    return spamMails, hamMails\n",
        "\n",
        "#functions to split training data\n",
        "def DivideTrainingData(spamMails, hamMails):\n",
        "\n",
        "\n",
        "    for doc in spamMails:\n",
        "        doc[\"DocumentClass\"] = 1\n",
        "        doc[\"W0\"] = 1\n",
        "\n",
        "    for doc in hamMails:\n",
        "        doc[\"DocumentClass\"] = 0\n",
        "        doc[\"W0\"] = 1\n",
        "\n",
        "    totalData = spamMails + hamMails\n",
        "    random.shuffle(totalData)\n",
        "\n",
        "    validation = totalData[0: int(len(totalData) * .3)]\n",
        "    training = totalData[int(len(totalData) * .3): -1]\n",
        "\n",
        "    return training, validation\n",
        "\n",
        "\n",
        "#helper function to get output which is used to classify\n",
        "def weightedSum(_input ,weight):\n",
        "\n",
        "    res = weight['W0'] * 1\n",
        "    for val in _input:\n",
        "        if val == 'DocumentClass' or val == 'W0':\n",
        "            continue\n",
        "        else:\n",
        "            if val in weight and val in _input:\n",
        "                res = res + (_input[val] * weight[val])\n",
        "    return res\n",
        "\n",
        "\n",
        "def TrainMCAPLR(training, vocabulary, learningRate, _lambda, iterations):\n",
        "\n",
        "\n",
        "    weight = copy.deepcopy(vocabulary)\n",
        "\n",
        "\n",
        "    for val in weight:\n",
        "        weight[val] = 0\n",
        "    weight['W0'] = 0\n",
        "    for val in range(iterations):\n",
        "        for instance in training:\n",
        "            try:\n",
        "              posterior = 1/(float(1 + np.exp(-weightedSum(instance, weight))))\n",
        "            except OverflowError:\n",
        "              posterior = 1\n",
        "            sum = 0\n",
        "            for w in weight:\n",
        "                if instance[w] != 0:\n",
        "                    if w == 'W0':\n",
        "                        sum = sum + learningRate * (\n",
        "                                instance[\"DocumentClass\"] - posterior)\n",
        "                    else:\n",
        "                        sum = sum + learningRate * (instance[w] * (instance[\"DocumentClass\"] - posterior))\n",
        "                    weight[w] = weight[w] + sum - learningRate * _lambda * weight[w]\n",
        "    return weight\n",
        "\n",
        "\n",
        "def TestMCAPLR(test_Input, weight):\n",
        "\n",
        "    res = weightedSum(weight, test_Input)\n",
        "    if res > 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "#finding optimal lambda by iterating over lambda from 1 to 10 to see which one yields the highest accuracy using validation data\n",
        "def ValidationMCAPLR(training, validation, vocabulary):\n",
        "\n",
        "    learningRate = 0.1\n",
        "    bestLambda = 2\n",
        "    maxAccuracy = 0\n",
        "    for _lambda in range(1, 10):\n",
        "        correctPredictions = 0\n",
        "        weight = TrainMCAPLR(training, vocabulary, learningRate, _lambda, 25)\n",
        "        for value in validation:\n",
        "            output = TestMCAPLR(value, weight)\n",
        "            if output == value[\"DocumentClass\"]:\n",
        "                correctPredictions = correctPredictions + 1\n",
        "        accuracy = correctPredictions / float(len(validation))\n",
        "        if accuracy > maxAccuracy:\n",
        "            maxAccuracy = accuracy\n",
        "            bestLambda = _lambda\n",
        "    return bestLambda\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def InitializeDocClass(spamMails, hamMails):\n",
        "    for doc in spamMails:\n",
        "        doc[\"DocumentClass\"] = 1\n",
        "    for doc in hamMails:\n",
        "        doc[\"DocumentClass\"] = 0\n",
        "    return spamMails, hamMails\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def ParameterTuner(validationX, validationY):\n",
        "    ParameterList = {'max_iter': (range(500, 2000, 1000)),'eta0': (0.2, 0.8),\n",
        "                              'tol': (0.002, 0.004),'alpha': (0.01, 0.03), 'penalty': ('l1', 'l2')\n",
        "                              }\n",
        "    SGDclassifier = SGDClassifier()\n",
        "    gridSearch = GridSearchCV(SGDclassifier, ParameterList, cv=5)\n",
        "    gridSearch.fit(validationX, validationY)\n",
        "    return gridSearch\n",
        "\n",
        "\n",
        "\n",
        "def dataConverter(data, words):\n",
        "    trainX = []\n",
        "    trainY = []\n",
        "    for value in data:\n",
        "        trainXVal = []\n",
        "        trainY.append(value[\"DocumentClass\"])\n",
        "        for word in words:\n",
        "            try:\n",
        "                trainXVal.append(value[word])\n",
        "            except:\n",
        "                trainXVal.append(0)\n",
        "        trainX.append(trainXVal)\n",
        "    return trainX, trainY\n",
        "\n",
        "def TestSGDC(trainedClassifier, testX, testY):\n",
        "\n",
        "    predictedVal = []\n",
        "    for value in testX:\n",
        "        predictedVal.append(trainedClassifier.predict(np.reshape(value, (1, -1))))\n",
        "    return predictedVal, testY\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def combineData(spamMails, hamMails):\n",
        "    totalData = spamMails + hamMails\n",
        "    return totalData\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KTeS8IQiLErU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Results**\n",
        "\n"
      ],
      "metadata": {
        "id": "_74LkWjrLwNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Bag of Words Multinomial NB\n",
        "\n"
      ],
      "metadata": {
        "id": "gfyvdlM2K-XY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) enron1"
      ],
      "metadata": {
        "id": "UfqKU5jZMyml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList=ImportDataset(\"enron1\",True)\n",
        "#importing training data and creating bag of words\n",
        "spamFrequencyList, hamFrequencyList, totalFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BagOfWords(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "\n",
        "\n",
        "#training MNB\n",
        "condProbability, condProbabilityOfAbsentWord, priors = TrainMultinomialNB(spamFrequencyList, hamFrequencyList, totalFrequencyList,\n",
        "                         spamVocab, hamVocab, sizeOfTotalFileList,\n",
        "                         sizeOfSpamList, sizeOfHamList, vocabulary)\n",
        "\n",
        "\n",
        "\n",
        "# #importing test data\n",
        "spamFilesTest, hamFilesTest, totalFilesTest, sizeOfTotalFileListTest, sizeOfSpamListTest, sizeOfHamListTest = ImportDataset(\"enron1\", False)\n",
        "\n",
        "\n",
        "#creating bag of words with test data\n",
        "spamFrequencyListTest, hamFrequencyListTest, totalFrequencyListTest, spamVocabTest, hamVocabTest, sizeOfTotalFileListTest, sizeOfSpamListTest, sizeOfHamListTest, vocabularyTest = BagOfWords(spamFilesTest, hamFilesTest, totalFilesTest, sizeOfTotalFileListTest, sizeOfSpamListTest, sizeOfHamListTest)\n",
        "#testing MNB\n",
        "predictedSpam = []\n",
        "for value in spamFrequencyListTest:\n",
        "    predictedSpam.append(TestMultinomialNB(priors, condProbability, condProbabilityOfAbsentWord, value))\n",
        "actualSpam=[1]*len(predictedSpam)\n",
        "predictedHam = []\n",
        "for value in hamFrequencyListTest:\n",
        "    predictedHam.append(TestMultinomialNB(priors, condProbability, condProbabilityOfAbsentWord, value))\n",
        "actualHam=[0]*len(predictedHam)\n",
        "totalActual=actualSpam+actualHam\n",
        "totalPredicted=predictedSpam+ predictedHam\n",
        "print(\"Accuracy: \",findAccuracy(totalPredicted,totalActual))\n",
        "print(\"Precision: \",findPrecision(totalPredicted,totalActual))\n",
        "print(\"Recall: \",findRecall(totalPredicted,totalActual))\n",
        "print(\"F1 Score: \",findF1_score(findRecall(totalPredicted,totalActual),findPrecision(totalPredicted,totalActual)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc5zTQfRK171",
        "outputId": "c9b987e7-4a8d-4d45-a4b6-6d1aea2d8d1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.9407894736842105\n",
            "Precision:  0.8860759493670886\n",
            "Recall:  0.9395973154362416\n",
            "F1 Score:  0.9120521172638436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) enron2, if running this gives you a divide by 0 error it means you have not made an enron2 folder and placed the train and test folders inside"
      ],
      "metadata": {
        "id": "ZVKFRZryUIiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing training data\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron2\", True)\n",
        "#creating bag of words\n",
        "spamFrequencyList, hamFrequencyList, totalFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BagOfWords(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "#training MNB\n",
        "condProbability, condProbabilityOfAbsentWord, priors = TrainMultinomialNB(spamFrequencyList, hamFrequencyList, totalFrequencyList,\n",
        "                         spamVocab, hamVocab, sizeOfTotalFileList,\n",
        "                         sizeOfSpamList, sizeOfHamList, vocabulary)\n",
        "#importing test data\n",
        "spamFilesTest, hamFilesTest, totalFilesTest, sizeOfTotalFileListTest, sizeOfSpamListTest, sizeOfHamListTest = ImportDataset(\"enron1\", False)\n",
        "#creating bag of words with test data\n",
        "spamFrequencyListTest, hamFrequencyListTest, totalFrequencyListTest, spamVocabTest, hamVocabTest, sizeOfTotalFileListTest, sizeOfSpamListTest, sizeOfHamListTest, vocabularyTest = BagOfWords(spamFilesTest, hamFilesTest, totalFilesTest, sizeOfTotalFileListTest, sizeOfSpamListTest, sizeOfHamListTest)\n",
        "#testing MNB\n",
        "predictedSpam = []\n",
        "for value in spamFrequencyListTest:\n",
        "    predictedSpam.append(TestMultinomialNB(priors, condProbability, condProbabilityOfAbsentWord, value))\n",
        "actualSpam=[1]*len(predictedSpam)\n",
        "predictedHam = []\n",
        "for value in hamFrequencyListTest:\n",
        "    predictedHam.append(TestMultinomialNB(priors, condProbability, condProbabilityOfAbsentWord, value))\n",
        "actualHam=[0]*len(predictedHam)\n",
        "totalActual=actualSpam+actualHam\n",
        "totalPredicted=predictedSpam+predictedHam\n",
        "print(\"Accuracy: \",findAccuracy(totalPredicted,totalActual))\n",
        "print(\"Precision: \",findPrecision(totalPredicted,totalActual))\n",
        "print(\"Recall: \",findRecall(totalPredicted,totalActual))\n",
        "print(\"F1 Score: \",findF1_score(findRecall(totalPredicted,totalActual),findPrecision(totalPredicted,totalActual)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJ76j12pUGBi",
        "outputId": "f365fa0b-905e-4d61-ead0-48aa9a0b2501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.956140350877193\n",
            "Precision:  0.910828025477707\n",
            "Recall:  0.959731543624161\n",
            "F1 Score:  0.934640522875817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) enron4"
      ],
      "metadata": {
        "id": "eI_KJMl7Umo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing training data\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron4\", True)\n",
        "#creating bag of words\n",
        "spamFrequencyList, hamFrequencyList, totalFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BagOfWords(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "#training MNB\n",
        "condProbability, condProbabilityOfAbsentWord, priors = TrainMultinomialNB(spamFrequencyList, hamFrequencyList, totalFrequencyList,\n",
        "                         spamVocab, hamVocab, sizeOfTotalFileList,\n",
        "                         sizeOfSpamList, sizeOfHamList, vocabulary)\n",
        "#importing test data\n",
        "spamFilesTest, hamFilesTest, totalFilesTest, sizeOfTotalFileListTest, sizeOfSpamListTest, sizeOfHamListTest = ImportDataset(\"enron1\", False)\n",
        "#creating bag of words with test data\n",
        "spamFrequencyListTest, hamFrequencyListTest, totalFrequencyListTest, spamVocabTest, hamVocabTest, sizeOfTotalFileListTest, sizeOfSpamListTest, sizeOfHamListTest, vocabularyTest = BagOfWords(spamFilesTest, hamFilesTest, totalFilesTest, sizeOfTotalFileListTest, sizeOfSpamListTest, sizeOfHamListTest)\n",
        "#testing MNB\n",
        "predictedSpam = []\n",
        "for value in spamFrequencyListTest:\n",
        "    predictedSpam.append(TestMultinomialNB(priors, condProbability, condProbabilityOfAbsentWord, value))\n",
        "actualSpam=[1]*len(predictedSpam)\n",
        "predictedHam = []\n",
        "for value in hamFrequencyListTest:\n",
        "    predictedHam.append(TestMultinomialNB(priors, condProbability, condProbabilityOfAbsentWord, value))\n",
        "actualHam=[0]*len(predictedHam)\n",
        "totalActual=actualSpam+actualHam\n",
        "totalPredicted=predictedSpam+predictedHam\n",
        "print(\"Accuracy: \",findAccuracy(totalPredicted,totalActual))\n",
        "print(\"Precision: \",findPrecision(totalPredicted,totalActual))\n",
        "print(\"Recall: \",findRecall(totalPredicted,totalActual))\n",
        "print(\"F1 Score: \",findF1_score(findRecall(totalPredicted,totalActual),findPrecision(totalPredicted,totalActual)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHy7HEOPUpgP",
        "outputId": "6fe43b01-a33a-415c-ef81-ab3b81f5b380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.8355263157894737\n",
            "Precision:  0.7371794871794872\n",
            "Recall:  0.7718120805369127\n",
            "F1 Score:  0.7540983606557377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Bernoulli Discrete NB"
      ],
      "metadata": {
        "id": "cn6Mbb5hXBgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "enron1"
      ],
      "metadata": {
        "id": "KmHR9LMGv6fQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing training data\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron1\", True)\n",
        "#creating bernoulli representation\n",
        "spamBernoulli, hamBernoulli, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BernoulliModel(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "#training Discrete NB\n",
        "condProbability, condProbabilityOfAbsentWord, priors = TrainDiscreteNB(spamBernoulli, hamBernoulli,\n",
        "                               spamVocab, hamVocab, sizeOfTotalFileList,\n",
        "                               sizeOfSpamList, sizeOfHamList, vocabulary)\n",
        "#importing test data\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron1\", False)\n",
        "#creating bernoulli representation with test data\n",
        "spamBernoulli, hamBernoulli, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BernoulliModel(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "#testing DNB\n",
        "predictedSpam = []\n",
        "for value in spamFrequencyListTest:\n",
        "    predictedSpam.append(TestDiscreteNB(priors, condProbability, condProbabilityOfAbsentWord, value))\n",
        "actualSpam=[1]*len(predictedSpam)\n",
        "predictedHam = []\n",
        "for value in hamFrequencyListTest:\n",
        "    predictedHam.append(TestDiscreteNB(priors, condProbability, condProbabilityOfAbsentWord, value))\n",
        "actualHam=[0]*len(predictedHam)\n",
        "totalActual=actualSpam+actualHam\n",
        "totalPredicted=predictedSpam+predictedHam\n",
        "print(\"Accuracy: \",findAccuracy(totalPredicted,totalActual))\n",
        "print(\"Precision: \",findPrecision(totalPredicted,totalActual))\n",
        "print(\"Recall: \",findRecall(totalPredicted,totalActual))\n",
        "print(\"F1 Score: \",findF1_score(findRecall(totalPredicted,totalActual),findPrecision(totalPredicted,totalActual)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuYAcPYmXE7I",
        "outputId": "ab875730-bbf4-44ca-871c-2c09c6692c06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.9342105263157895\n",
            "Precision:  0.8993288590604027\n",
            "Recall:  0.8993288590604027\n",
            "F1 Score:  0.8993288590604027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "enron2"
      ],
      "metadata": {
        "id": "C2NwO2qBv4oJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing training data\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron2\", True)\n",
        "#creating bernoulli representation\n",
        "spamBernoulli, hamBernoulli, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BernoulliModel(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "#training Discrete NB\n",
        "condProbability, condProbabilityOfAbsentWord, priors = TrainDiscreteNB(spamBernoulli, hamBernoulli,\n",
        "                               spamVocab, hamVocab, sizeOfTotalFileList,\n",
        "                               sizeOfSpamList, sizeOfHamList, vocabulary)\n",
        "#importing test data\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron2\", False)\n",
        "#creating bernoulli representation with test data\n",
        "spamBernoulli, hamBernoulli, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BernoulliModel(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "#testing DNB\n",
        "predictedSpam = []\n",
        "for value in spamFrequencyListTest:\n",
        "    predictedSpam.append(TestDiscreteNB(priors, condProbability, condProbabilityOfAbsentWord, value))\n",
        "actualSpam=[1]*len(predictedSpam)\n",
        "predictedHam = []\n",
        "for value in hamFrequencyListTest:\n",
        "    predictedHam.append(TestDiscreteNB(priors, condProbability, condProbabilityOfAbsentWord, value))\n",
        "actualHam=[0]*len(predictedHam)\n",
        "totalActual=actualSpam+actualHam\n",
        "totalPredicted=predictedSpam+predictedHam\n",
        "print(\"Accuracy: \",findAccuracy(totalPredicted,totalActual))\n",
        "print(\"Precision: \",findPrecision(totalPredicted,totalActual))\n",
        "print(\"Recall: \",findRecall(totalPredicted,totalActual))\n",
        "print(\"F1 Score: \",findF1_score(findRecall(totalPredicted,totalActual),findPrecision(totalPredicted,totalActual)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHTFJShbv3_U",
        "outputId": "1ff7674a-7312-4b3a-ebf4-1d18eb8f6818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.9627192982456141\n",
            "Precision:  0.9583333333333334\n",
            "Recall:  0.9261744966442953\n",
            "F1 Score:  0.9419795221843005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "enron 4"
      ],
      "metadata": {
        "id": "ZlXraBnCwEGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing training data\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron4\", True)\n",
        "#creating bernoulli representation\n",
        "spamBernoulli, hamBernoulli, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BernoulliModel(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "#training Discrete NB\n",
        "condProbability, condProbabilityOfAbsentWord, priors = TrainDiscreteNB(spamBernoulli, hamBernoulli,\n",
        "                               spamVocab, hamVocab, sizeOfTotalFileList,\n",
        "                               sizeOfSpamList, sizeOfHamList, vocabulary)\n",
        "#importing test data\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron4\", False)\n",
        "#creating bernoulli representation with test data\n",
        "spamBernoulli, hamBernoulli, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BernoulliModel(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "#testing DNB\n",
        "predictedSpam = []\n",
        "for value in spamFrequencyListTest:\n",
        "    predictedSpam.append(TestDiscreteNB(priors, condProbability, condProbabilityOfAbsentWord, value))\n",
        "actualSpam=[1]*len(predictedSpam)\n",
        "predictedHam = []\n",
        "for value in hamFrequencyListTest:\n",
        "    predictedHam.append(TestDiscreteNB(priors, condProbability, condProbabilityOfAbsentWord, value))\n",
        "actualHam=[0]*len(predictedHam)\n",
        "totalActual=actualSpam+actualHam\n",
        "totalPredicted=predictedSpam+predictedHam\n",
        "print(\"Accuracy: \",findAccuracy(totalPredicted,totalActual))\n",
        "print(\"Precision: \",findPrecision(totalPredicted,totalActual))\n",
        "print(\"Recall: \",findRecall(totalPredicted,totalActual))\n",
        "print(\"F1 Score: \",findF1_score(findRecall(totalPredicted,totalActual),findPrecision(totalPredicted,totalActual)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAXHSWTNwGFi",
        "outputId": "2aff9824-f88b-47b8-f1a9-60bb74834076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.5986842105263158\n",
            "Precision:  0.4472049689440994\n",
            "Recall:  0.9664429530201343\n",
            "F1 Score:  0.6114649681528662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) MCAP Logistic Regression"
      ],
      "metadata": {
        "id": "F9z-0XhNwKZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 Bag of words"
      ],
      "metadata": {
        "id": "38gzJR3TwY10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "enron1"
      ],
      "metadata": {
        "id": "52T7kfdiwd5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing training data\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron1\", True)\n",
        "#converting to BOW\n",
        "spamFrequencyList, hamFrequencyList, totalFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BagOfWords(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "#initialize w0\n",
        "spamFrequencyList,hamFrequencyList=InitializeW0(spamFrequencyList,hamFrequencyList)\n",
        "#splitting training data into training and validation\n",
        "training, validation = DivideTrainingData(spamFrequencyList, hamFrequencyList)\n",
        "\n"
      ],
      "metadata": {
        "id": "ilM0kWnkwUNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training MCAP LR\n",
        "bestLambda = ValidationMCAPLR(training, validation, vocabulary)\n",
        "weight = TrainMCAPLR(training, vocabulary, 0.1, bestLambda, 100)\n",
        "\n",
        "#importing testing data\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron1\", False)\n",
        "spamFrequencyList, hamFrequencyList, totalFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BagOfWords(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "UzisT8PC5lX1",
        "outputId": "5153ee76-5988-4318-9a73-b6306809275e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-111-9f90305b466b>:384: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (float(1 + np.exp(-value)))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'W0'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-8d7184d1ad72>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mspamPredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspamFrequencyList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mspamPredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTestMCAPLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mhamPredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-111-9f90305b466b>\u001b[0m in \u001b[0;36mTestMCAPLR\u001b[0;34m(test_Input, weight)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mTestMCAPLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_Input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweightedSum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_Input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-111-9f90305b466b>\u001b[0m in \u001b[0;36mweightedSum\u001b[0;34m(_input, weight)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mweightedSum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_input\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W0'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DocumentClass'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'W0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'W0'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spamFrequencyList,hamFrequencyList=InitializeW0(spamFrequencyList,hamFrequencyList)\n",
        "#testing weights\n",
        "spamPredicted = []\n",
        "for value in spamFrequencyList:\n",
        "    spamPredicted.append(TestMCAPLR(value, weight))\n",
        "\n",
        "hamPredicted = []\n",
        "for value in hamFrequencyList:\n",
        "    hamPredicted.append(TestMCAPLR(value, weight))\n",
        "\n",
        "actualSpam = [1] * len(spamPredicted)\n",
        "actualHam = [0] * len(hamPredicted)\n",
        "totalActual = actualSpam + actualHam\n",
        "totalPredicted = spamPredicted + hamPredicted\n",
        "\n",
        "print(\"Accuracy: \", findAccuracy(totalPredicted, totalActual))\n",
        "print(\"Precision: \", findPrecision(totalPredicted, totalActual))\n",
        "print(\"Recall: \", findRecall(totalPredicted, totalActual))\n",
        "print(\"F1 Score: \", findF1_score(findRecall(totalPredicted, totalActual), findPrecision(totalPredicted, totalActual)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzvz16P4CMjA",
        "outputId": "88b38bd1-16b4-4398-9f3f-7a62c3b76e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.9298245614035088\n",
            "Precision:  0.8305084745762712\n",
            "Recall:  0.9865771812080537\n",
            "F1 Score:  0.901840490797546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "enron2"
      ],
      "metadata": {
        "id": "HgRIa09DCU1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing training data\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron2\", True)\n",
        "#converting to BOW\n",
        "spamFrequencyList, hamFrequencyList, totalFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BagOfWords(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "#initialize w0\n",
        "spamFrequencyList,hamFrequencyList=InitializeW0(spamFrequencyList,hamFrequencyList)\n",
        "#splitting training data into training and validation\n",
        "training, validation = DivideTrainingData(spamFrequencyList, hamFrequencyList)\n",
        "\n",
        "#training MCAP LR\n",
        "bestLambda = ValidationMCAPLR(training, validation, vocabulary)\n",
        "weight = TrainMCAPLR(training, vocabulary, 0.1, bestLambda, 100)\n",
        "\n",
        "#importing testing data\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron2\", False)\n",
        "spamFrequencyList, hamFrequencyList, totalFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BagOfWords(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "\n",
        "spamFrequencyList,hamFrequencyList=InitializeW0(spamFrequencyList,hamFrequencyList)\n",
        "#testing weights\n",
        "spamPredicted = []\n",
        "for value in spamFrequencyList:\n",
        "    spamPredicted.append(TestMCAPLR(value, weight))\n",
        "\n",
        "hamPredicted = []\n",
        "for value in hamFrequencyList:\n",
        "    hamPredicted.append(TestMCAPLR(value, weight))\n",
        "\n",
        "actualSpam = [1] * len(spamPredicted)\n",
        "actualHam = [0] * len(hamPredicted)\n",
        "totalActual = actualSpam + actualHam\n",
        "totalPredicted = spamPredicted + hamPredicted\n",
        "\n",
        "print(\"Accuracy: \", findAccuracy(totalPredicted, totalActual))\n",
        "print(\"Precision: \", findPrecision(totalPredicted, totalActual))\n",
        "print(\"Recall: \", findRecall(totalPredicted, totalActual))\n",
        "print(\"F1 Score: \", findF1_score(findRecall(totalPredicted, totalActual), findPrecision(totalPredicted, totalActual)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVeqka6cCSIx",
        "outputId": "c954ca0d-629b-40e5-91b3-ac38a5664d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-111-9f90305b466b>:384: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (float(1 + np.exp(-value)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.899581589958159\n",
            "Precision:  0.735632183908046\n",
            "Recall:  0.9846153846153847\n",
            "F1 Score:  0.8421052631578947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "enron4"
      ],
      "metadata": {
        "id": "M440daxxDNTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing training data\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron4\", True)\n",
        "#converting to BOW\n",
        "spamFrequencyList, hamFrequencyList, totalFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BagOfWords(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "#initialize w0\n",
        "spamFrequencyList,hamFrequencyList=InitializeW0(spamFrequencyList,hamFrequencyList)\n",
        "#splitting training data into training and validation\n",
        "training, validation = DivideTrainingData(spamFrequencyList, hamFrequencyList)\n",
        "\n",
        "#training MCAP LR\n",
        "bestLambda = ValidationMCAPLR(training, validation, vocabulary)\n",
        "weight = TrainMCAPLR(training, vocabulary, 0.1, bestLambda, 100)\n",
        "\n",
        "#importing testing data\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron4\", False)\n",
        "spamFrequencyList, hamFrequencyList, totalFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BagOfWords(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "\n",
        "spamFrequencyList,hamFrequencyList=InitializeW0(spamFrequencyList,hamFrequencyList)\n",
        "#testing weights\n",
        "spamPredicted = []\n",
        "for value in spamFrequencyList:\n",
        "    spamPredicted.append(TestMCAPLR(value, weight))\n",
        "\n",
        "hamPredicted = []\n",
        "for value in hamFrequencyList:\n",
        "    hamPredicted.append(TestMCAPLR(value, weight))\n",
        "\n",
        "actualSpam = [1] * len(spamPredicted)\n",
        "actualHam = [0] * len(hamPredicted)\n",
        "totalActual = actualSpam + actualHam\n",
        "totalPredicted = spamPredicted + hamPredicted\n",
        "\n",
        "print(\"Accuracy: \", findAccuracy(totalPredicted, totalActual))\n",
        "print(\"Precision: \", findPrecision(totalPredicted, totalActual))\n",
        "print(\"Recall: \", findRecall(totalPredicted, totalActual))\n",
        "print(\"F1 Score: \", findF1_score(findRecall(totalPredicted, totalActual), findPrecision(totalPredicted, totalActual)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJ1_EbrJDKgF",
        "outputId": "a159e1f1-400b-45c5-fa6a-341a79925c95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-115-b6a25ea57bd8>:387: RuntimeWarning: overflow encountered in exp\n",
            "  posterior = 1/(float(1 + np.exp(-weightedSum(instance, weight))))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.6832412523020258\n",
            "Precision:  0.9823788546255506\n",
            "Recall:  0.5703324808184144\n",
            "F1 Score:  0.7216828478964401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2 Bernoulli Model"
      ],
      "metadata": {
        "id": "CZP-ni3aDkHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "enron1"
      ],
      "metadata": {
        "id": "n_BCgQv-Dp5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing training data\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron1\", True)\n",
        "#converting to Bernoulli\n",
        "\n",
        "spamFrequencyList, hamFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BernoulliModel(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "#initialize w0\n",
        "spamFrequencyList,hamFrequencyList=InitializeW0(spamFrequencyList,hamFrequencyList)\n",
        "#splitting training data into training and validation\n",
        "training, validation = DivideTrainingData(spamFrequencyList, hamFrequencyList)\n",
        "\n",
        "#training MCAP LR\n",
        "bestLambda = ValidationMCAPLR(training, validation, vocabulary)\n",
        "weight = TrainMCAPLR(training, vocabulary, 0.1, bestLambda, 100)\n",
        "\n",
        "#importing testing data\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron1\", False)\n",
        "spamFrequencyList, hamFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BernoulliModel(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "\n",
        "spamFrequencyList,hamFrequencyList=InitializeW0(spamFrequencyList,hamFrequencyList)\n",
        "#testing weights\n",
        "spamPredicted = []\n",
        "for value in spamFrequencyList:\n",
        "    spamPredicted.append(TestMCAPLR(value, weight))\n",
        "\n",
        "hamPredicted = []\n",
        "for value in hamFrequencyList:\n",
        "    hamPredicted.append(TestMCAPLR(value, weight))\n",
        "\n",
        "actualSpam = [1] * len(spamPredicted)\n",
        "actualHam = [0] * len(hamPredicted)\n",
        "totalActual = actualSpam + actualHam\n",
        "totalPredicted = spamPredicted + hamPredicted\n",
        "\n",
        "print(\"Accuracy: \", findAccuracy(totalPredicted, totalActual))\n",
        "print(\"Precision: \", findPrecision(totalPredicted, totalActual))\n",
        "print(\"Recall: \", findRecall(totalPredicted, totalActual))\n",
        "print(\"F1 Score: \", findF1_score(findRecall(totalPredicted, totalActual), findPrecision(totalPredicted, totalActual)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9X4Ah-CFDnax",
        "outputId": "f5ce36d9-b993-4724-b400-0df2cb6fd16a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-115-b6a25ea57bd8>:387: RuntimeWarning: overflow encountered in exp\n",
            "  posterior = 1/(float(1 + np.exp(-weightedSum(instance, weight))))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.9210526315789473\n",
            "Precision:  0.8228571428571428\n",
            "Recall:  0.9664429530201343\n",
            "F1 Score:  0.8888888888888888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "enron2"
      ],
      "metadata": {
        "id": "91DPoW9CEfLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing training data\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron2\", True)\n",
        "#converting to Bernoulli\n",
        "\n",
        "spamFrequencyList, hamFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BernoulliModel(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "#initialize w0\n",
        "spamFrequencyList,hamFrequencyList=InitializeW0(spamFrequencyList,hamFrequencyList)\n",
        "#splitting training data into training and validation\n",
        "training, validation = DivideTrainingData(spamFrequencyList, hamFrequencyList)\n",
        "\n",
        "#training MCAP LR\n",
        "bestLambda = ValidationMCAPLR(training, validation, vocabulary)\n",
        "weight = TrainMCAPLR(training, vocabulary, 0.1, bestLambda, 100)\n",
        "\n",
        "#importing testing data\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron2\", False)\n",
        "spamFrequencyList, hamFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BernoulliModel(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "\n",
        "spamFrequencyList,hamFrequencyList=InitializeW0(spamFrequencyList,hamFrequencyList)\n",
        "#testing weights\n",
        "spamPredicted = []\n",
        "for value in spamFrequencyList:\n",
        "    spamPredicted.append(TestMCAPLR(value, weight))\n",
        "\n",
        "hamPredicted = []\n",
        "for value in hamFrequencyList:\n",
        "    hamPredicted.append(TestMCAPLR(value, weight))\n",
        "\n",
        "actualSpam = [1] * len(spamPredicted)\n",
        "actualHam = [0] * len(hamPredicted)\n",
        "totalActual = actualSpam + actualHam\n",
        "totalPredicted = spamPredicted + hamPredicted\n",
        "\n",
        "print(\"Accuracy: \", findAccuracy(totalPredicted, totalActual))\n",
        "print(\"Precision: \", findPrecision(totalPredicted, totalActual))\n",
        "print(\"Recall: \", findRecall(totalPredicted, totalActual))\n",
        "print(\"F1 Score: \", findF1_score(findRecall(totalPredicted, totalActual), findPrecision(totalPredicted, totalActual)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSeuLXkoEeiw",
        "outputId": "f8e76d32-f729-447a-cd5a-4e1433998582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-115-b6a25ea57bd8>:387: RuntimeWarning: overflow encountered in exp\n",
            "  posterior = 1/(float(1 + np.exp(-weightedSum(instance, weight))))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.9121338912133892\n",
            "Precision:  0.775\n",
            "Recall:  0.9538461538461539\n",
            "F1 Score:  0.8551724137931034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "enron4"
      ],
      "metadata": {
        "id": "GIW_fO9AEpf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing training data\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron4\", True)\n",
        "#converting to Bernoulli\n",
        "\n",
        "spamFrequencyList, hamFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BernoulliModel(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "#initialize w0\n",
        "spamFrequencyList,hamFrequencyList=InitializeW0(spamFrequencyList,hamFrequencyList)\n",
        "#splitting training data into training and validation\n",
        "training, validation = DivideTrainingData(spamFrequencyList, hamFrequencyList)\n",
        "\n",
        "#training MCAP LR\n",
        "bestLambda = ValidationMCAPLR(training, validation, vocabulary)\n",
        "weight = TrainMCAPLR(training, vocabulary, 0.1, bestLambda, 100)\n",
        "\n",
        "#importing testing data\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron4\", False)\n",
        "spamFrequencyList, hamFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BernoulliModel(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "\n",
        "spamFrequencyList,hamFrequencyList=InitializeW0(spamFrequencyList,hamFrequencyList)\n",
        "#testing weights\n",
        "spamPredicted = []\n",
        "for value in spamFrequencyList:\n",
        "    spamPredicted.append(TestMCAPLR(value, weight))\n",
        "\n",
        "hamPredicted = []\n",
        "for value in hamFrequencyList:\n",
        "    hamPredicted.append(TestMCAPLR(value, weight))\n",
        "\n",
        "actualSpam = [1] * len(spamPredicted)\n",
        "actualHam = [0] * len(hamPredicted)\n",
        "totalActual = actualSpam + actualHam\n",
        "totalPredicted = spamPredicted + hamPredicted\n",
        "\n",
        "print(\"Accuracy: \", findAccuracy(totalPredicted, totalActual))\n",
        "print(\"Precision: \", findPrecision(totalPredicted, totalActual))\n",
        "print(\"Recall: \", findRecall(totalPredicted, totalActual))\n",
        "print(\"F1 Score: \", findF1_score(findRecall(totalPredicted, totalActual), findPrecision(totalPredicted, totalActual)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWj4sUGSElks",
        "outputId": "5f438b9e-cbb2-496c-842c-607be75e9f64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-115-b6a25ea57bd8>:387: RuntimeWarning: overflow encountered in exp\n",
            "  posterior = 1/(float(1 + np.exp(-weightedSum(instance, weight))))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.9337016574585635\n",
            "Precision:  0.9156908665105387\n",
            "Recall:  1.0\n",
            "F1 Score:  0.9559902200488998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) SGD Classifier from Scikit-learn"
      ],
      "metadata": {
        "id": "5G9F0AUTEvNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.1 Bag of words model"
      ],
      "metadata": {
        "id": "35JAeP0FE1-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "enron1"
      ],
      "metadata": {
        "id": "wvN1-HSHNOQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron1\",True)\n",
        "#creating bag of words of training data\n",
        "spamFrequencyList, hamFrequencyList, totalFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BagOfWords(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "#initializing document classes\n",
        "spamFrequencyList,hamFrequencyList=InitializeDocClass(spamFrequencyList,hamFrequencyList)\n",
        "trainData, validation = DivideTrainingData(spamFrequencyList, hamFrequencyList)\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron1\",False)\n",
        "#creating bag of words of testing data\n",
        "spamFrequencyList, hamFrequencyList, totalFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BagOfWords(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "spamFrequencyList,hamFrequencyList=InitializeDocClass(spamFrequencyList,hamFrequencyList)\n",
        "testData=combineData(spamFrequencyList,hamFrequencyList)\n",
        "wordList=list(trainData[0])\n",
        "trainX,trainY=dataConverter(trainData,wordList)\n",
        "testX,testY=dataConverter(testData,wordList)\n",
        "validationX,validationY=dataConverter(validation,wordList)\n",
        "\n",
        "classifier=ParameterTuner(validationX,validationY)\n",
        "\n",
        "trainedModel=classifier.fit(trainX, trainY)\n",
        "\n",
        "predictedY, actualY=TestSGDC(trainedModel,testX,testY)\n",
        "print(\"Accuracy: \",findAccuracy(predictedY,testY))\n",
        "print(\"Precision: \",findPrecision(predictedY,testY))\n",
        "print(\"Recall: \",findRecall(predictedY,testY))\n",
        "print(\"F1 Score: \",findF1_score(findRecall(predictedY,testY),findPrecision(predictedY,testY)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dmo4hEkOEzsN",
        "outputId": "184b616f-7da4-4aa9-9334-e5eb9b1e0639"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.9824561403508771\n",
            "Precision:  0.9731543624161074\n",
            "Recall:  0.9731543624161074\n",
            "F1 Score:  0.9731543624161074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "enron2"
      ],
      "metadata": {
        "id": "lSNU2J-oNMtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron2\",True)\n",
        "#creating bag of words of training data\n",
        "spamFrequencyList, hamFrequencyList, totalFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BagOfWords(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "#initializing document classes\n",
        "spamFrequencyList,hamFrequencyList=InitializeDocClass(spamFrequencyList,hamFrequencyList)\n",
        "trainData, validation = DivideTrainingData(spamFrequencyList, hamFrequencyList)\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron2\",False)\n",
        "#creating bag of words of testing data\n",
        "spamFrequencyList, hamFrequencyList, totalFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BagOfWords(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "spamFrequencyList,hamFrequencyList=InitializeDocClass(spamFrequencyList,hamFrequencyList)\n",
        "testData=combineData(spamFrequencyList,hamFrequencyList)\n",
        "trainX,trainY=dataConverter(trainData,wordList)\n",
        "testX,testY=dataConverter(testData,wordList)\n",
        "\n",
        "validationX,validationY=dataConverter(validation,wordList)\n",
        "\n",
        "classifier=ParameterTuner(validationX,validationY)\n",
        "\n",
        "trainedModel=classifier.fit(trainX, trainY)\n",
        "\n",
        "predictedY, actualY=TestSGDC(trainedModel,testX,testY)\n",
        "print(\"Accuracy: \",findAccuracy(predictedY,testY))\n",
        "print(\"Precision: \",findPrecision(predictedY,testY))\n",
        "print(\"Recall: \",findRecall(predictedY,testY))\n",
        "print(\"F1 Score: \",findF1_score(findRecall(predictedY,testY),findPrecision(predictedY,testY)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNnLAUWwNQnM",
        "outputId": "2df469b6-08e6-4d6a-b55c-1bb5f2b13ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.9853556485355649\n",
            "Precision:  0.9624060150375939\n",
            "Recall:  0.9846153846153847\n",
            "F1 Score:  0.973384030418251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "enron4"
      ],
      "metadata": {
        "id": "s01rkSlTNU8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron4\",True)\n",
        "#creating bag of words of training data\n",
        "spamFrequencyList, hamFrequencyList, totalFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BagOfWords(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "#initializing document classes\n",
        "spamFrequencyList,hamFrequencyList=InitializeDocClass(spamFrequencyList,hamFrequencyList)\n",
        "trainData, validation = DivideTrainingData(spamFrequencyList, hamFrequencyList)\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron4\",False)\n",
        "#creating bag of words of testing data\n",
        "spamFrequencyList, hamFrequencyList, totalFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BagOfWords(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "spamFrequencyList,hamFrequencyList=InitializeDocClass(spamFrequencyList,hamFrequencyList)\n",
        "testData=combineData(spamFrequencyList,hamFrequencyList)\n",
        "wordList=list(trainData[0])\n",
        "\n",
        "trainX,trainY=dataConverter(trainData,wordList)\n",
        "testX,testY=dataConverter(testData,wordList)\n",
        "validationX,validationY=dataConverter(validation,wordList)\n",
        "\n",
        "classifier=ParameterTuner(validationX,validationY)\n",
        "\n",
        "trainedModel=classifier.fit(trainX, trainY)\n",
        "\n",
        "predictedY, actualY=TestSGDC(trainedModel,testX,testY)\n",
        "print(\"Accuracy: \",findAccuracy(predictedY,testY))\n",
        "print(\"Precision: \",findPrecision(predictedY,testY))\n",
        "print(\"Recall: \",findRecall(predictedY,testY))\n",
        "print(\"F1 Score: \",findF1_score(findRecall(predictedY,testY),findPrecision(predictedY,testY)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHX5RIalNUox",
        "outputId": "c80d38ab-3416-453b-d37e-ece55c773944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.992633517495396\n",
            "Precision:  0.9898734177215189\n",
            "Recall:  1.0\n",
            "F1 Score:  0.9949109414758269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.2 Bernoulli Model"
      ],
      "metadata": {
        "id": "pEaD76FNNbir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "enron1"
      ],
      "metadata": {
        "id": "dhnhStp4Necw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron1\",True)\n",
        "#creating bernoulli representation of training data\n",
        "spamFrequencyList, hamFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BernoulliModel(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "#initializing document classes\n",
        "spamFrequencyList,hamFrequencyList=InitializeDocClass(spamFrequencyList,hamFrequencyList)\n",
        "trainData, validation = DivideTrainingData(spamFrequencyList, hamFrequencyList)\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron1\",False)\n",
        "#creating bag of words of testing data\n",
        "spamFrequencyList, hamFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BernoulliModel(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "spamFrequencyList,hamFrequencyList=InitializeDocClass(spamFrequencyList,hamFrequencyList)\n",
        "testData=combineData(spamFrequencyList,hamFrequencyList)\n",
        "wordList=list(trainData[0])\n",
        "trainX,trainY=dataConverter(trainData,wordList)\n",
        "testX,testY=dataConverter(testData,wordList)\n",
        "validationX,validationY=dataConverter(validation,wordList)\n",
        "\n",
        "classifier=ParameterTuner(validationX,validationY)\n",
        "#training the model based on the tuned parameters\n",
        "trainedModel=classifier.fit(trainX, trainY)\n",
        "\n",
        "predictedY, actualY=TestSGDC(trainedModel,testX,testY)\n",
        "print(\"Accuracy: \",findAccuracy(predictedY,testY))\n",
        "print(\"Precision: \",findPrecision(predictedY,testY))\n",
        "print(\"Recall: \",findRecall(predictedY,testY))\n",
        "print(\"F1 Score: \",findF1_score(findRecall(predictedY,testY),findPrecision(predictedY,testY)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmWp1dmJNhbM",
        "outputId": "62415cbb-30b2-4560-eedc-4daa1f68c286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  1.0\n",
            "Precision:  1.0\n",
            "Recall:  1.0\n",
            "F1 Score:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "enron2"
      ],
      "metadata": {
        "id": "v_AmNecvNvMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron2\",True)\n",
        "#creating bag of words of training data\n",
        "spamFrequencyList, hamFrequencyList, totalFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BagOfWords(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "#initializing document classes\n",
        "spamFrequencyList,hamFrequencyList=InitializeDocClass(spamFrequencyList,hamFrequencyList)\n",
        "trainData, validation = DivideTrainingData(spamFrequencyList, hamFrequencyList)\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron2\",False)\n",
        "#creating bag of words of testing data\n",
        "spamFrequencyList, hamFrequencyList, totalFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BagOfWords(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "spamFrequencyList,hamFrequencyList=InitializeDocClass(spamFrequencyList,hamFrequencyList)\n",
        "testData=combineData(spamFrequencyList,hamFrequencyList)\n",
        "wordList=list(trainData[0])\n",
        "trainX,trainY=dataConverter(trainData,wordList)\n",
        "testX,testY=dataConverter(testData,wordList)\n",
        "validationX,validationY=dataConverter(validation,wordList)\n",
        "\n",
        "classifier=ParameterTuner(validationX,validationY)\n",
        "\n",
        "trainedModel=classifier.fit(trainX, trainY)\n",
        "\n",
        "predictedY, actualY=TestSGDC(trainedModel,testX,testY)\n",
        "print(\"Accuracy: \",findAccuracy(predictedY,testY))\n",
        "print(\"Precision: \",findPrecision(predictedY,testY))\n",
        "print(\"Recall: \",findRecall(predictedY,testY))\n",
        "print(\"F1 Score: \",findF1_score(findRecall(predictedY,testY),findPrecision(predictedY,testY)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aEPGllPNyiM",
        "outputId": "aed09d9e-dcdf-4681-d7d6-6376db935f24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.997907949790795\n",
            "Precision:  1.0\n",
            "Recall:  0.9923076923076923\n",
            "F1 Score:  0.9961389961389961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "enron4"
      ],
      "metadata": {
        "id": "HGz_wbkdN3dQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron4\",True)\n",
        "#creating bag of words of training data\n",
        "spamFrequencyList, hamFrequencyList, totalFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BagOfWords(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "#initializing document classes\n",
        "spamFrequencyList,hamFrequencyList=InitializeDocClass(spamFrequencyList,hamFrequencyList)\n",
        "trainData, validation = DivideTrainingData(spamFrequencyList, hamFrequencyList)\n",
        "spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList = ImportDataset(\"enron4\",False)\n",
        "#creating bag of words of testing data\n",
        "spamFrequencyList, hamFrequencyList, totalFrequencyList, spamVocab, hamVocab, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList, vocabulary = BagOfWords(spamFiles, hamFiles, totalFiles, sizeOfTotalFileList, sizeOfSpamList, sizeOfHamList)\n",
        "spamFrequencyList,hamFrequencyList=InitializeDocClass(spamFrequencyList,hamFrequencyList)\n",
        "testData=combineData(spamFrequencyList,hamFrequencyList)\n",
        "wordList=list(trainData[0])\n",
        "trainX,trainY=dataConverter(trainData,wordList)\n",
        "testX,testY=dataConverter(testData,wordList)\n",
        "validationX,validationY=dataConverter(validation,wordList)\n",
        "\n",
        "classifier=ParameterTuner(validationX,validationY)\n",
        "\n",
        "trainedModel=classifier.fit(trainX, trainY)\n",
        "\n",
        "predictedY, actualY=TestSGDC(trainedModel,testX,testY)\n",
        "print(\"Accuracy: \",findAccuracy(predictedY,testY))\n",
        "print(\"Precision: \",findPrecision(predictedY,testY))\n",
        "print(\"Recall: \",findRecall(predictedY,testY))\n",
        "print(\"F1 Score: \",findF1_score(findRecall(predictedY,testY),findPrecision(predictedY,testY)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV3tC9gCN4ik",
        "outputId": "32b1b645-3ac9-4ce0-a6a6-eb992cfbeb31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.9705340699815838\n",
            "Precision:  0.9629629629629629\n",
            "Recall:  0.9974424552429667\n",
            "F1 Score:  0.9798994974874371\n"
          ]
        }
      ]
    }
  ]
}